train_Y = tec_train[,101]
test_Y = tec_test[,101]
#fitting the ridge regression model
lambdas = 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(temp_train_X, train_Y, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
summary(ridge_reg)
#performing cross validation
ridge_crossval <- cv.glmnet(temp_train_X, train_Y, alpha = 0, lambda = lambdas)
#performing cross validation
ridge_crossval <- cv.glmnet(temp_train_X, train_Y, alpha = 0, lambda = lambdas)
#thus we used the functions provided by the glmnet package to figure out the most optimal lambda
optimal_lambda <- ridge_crossval$lambda.min
optimal_lambda
#use the optimal lambda to predict the y_fat values and calculate MSE
rmse = function(x,y) {sqrt(mean((x-y)^2))}
mse = function(x,y) {mean((x-y)^2)}
ss_res_ridge = ss_res(test_Y, y_hat_ridge)
ss_tot_ridge = ss_tot(y_hat_ridge)
r2_ridge = r2(ss_res = ss_res_ridge,ss_tot = ss_tot_ridge)
mse_pred_ridge = mse(y_hat_ridge, test_Y)
mse_pred_ridge
r2_ridge
lambdas <- 10^seq(2, -3, by = -.1)
#alpha=1 for lasso regression
lasso_reg = cv.glmnet(temp_train_X, train_Y, alpha =1, lambda = lambdas, standardize=T)
#alpha=1 for lasso regression
lasso_reg = cv.glmnet(temp_train_X, train_Y, alpha =1, lambda = lambdas, standardize=T)
optimal_lambda_lasso = lasso_reg$lambda.min
optimal_lambda_lasso
#using the optimal lambda to predict the values of y_fat
y_hat_lasso = predict(lasso_reg, s = optimal_lambda_lasso, newx = temp_test_X)
#mean squared error being calcualted over the testing dataset
mse_pred_ridge = mse(y_hat_lasso, test_Y)
mse_pred_ridge
#calculating the r2 for this method
ss_res_lasso = ss_res(test_Y, y_hat_lasso)
ss_tot_lasso = ss_tot(y_hat_lasso)
r2_lasso = r2(ss_res = ss_res_lasso,ss_tot = ss_tot_lasso)
#mean squared error being calcualted over the testing dataset
mse_pred_ridge = mse(y_hat_lasso, test_Y)
mse_pred_ridge
r2_lasso
#mean squared error being calcualted over the testing dataset
mse_pred_lasso = mse(y_hat_lasso, test_Y)
mse_pred_lasso
r2_lasso
library("plsr")
library("spls")
library("tidyverse")
library("caret")
#predicting the values using 30 principal components
yhat = predict(pcr_fit, tec_test, ncomp=5)
# calculating the mse
mse_pred <- rmse(tec_test$y, yhat)^2
ss_res_pcr = ss_res(tec_test$y, yhat)
ss_tot_pcr = ss_tot(yhat)
r2_pcr = r2(ss_res_pcr, ss_tot_pcr)
r2_pcr
rmse(tec_test$y, yhat)
#predicting the values using 30 principal components
yhat = predict(pcr_fit, tec_test, ncomp=8)
# calculating the mse
mse_pred <- rmse(tec_test$y, yhat)^2
ss_res_pcr = ss_res(tec_test$y, yhat)
ss_tot_pcr = ss_tot(yhat)
r2_pcr = r2(ss_res_pcr, ss_tot_pcr)
r2_pcr
rmse(tec_test$y, yhat)
r2_pcr
#load the prostate dataset
data("prostate")
#preparing the data
prostate = na.omit(prostate)
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 70)
train_pros_idx
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
#trying out the sparse model
slg_model_cv = cv.glmnet(pros_trainX, pros_trainY, family="binomial",alpha=1)
optimal_lambda_slgr = slg_model_cv$lambda.min
optimal_lambda_slgr
#using the optimal value of lambda to build the classifier
slgr_model = glmnet(pros_trainX, pros_trainY, alpha = 1, family = "binomial",
lambda = optimal_lambda_slgr)
coef(slgr_model)
yhat_slgr_vals = predict(slgr_model, newx = pros_testX, type="class")
yhat_slgr_vals
yhat_slgr_vals = predict(slgr_model, newx = pros_testX, type="response")
yhat_slgr_vals
#type="response" gives the probabilite
yhat_slgr_vals = predict(slgr_model, newx = pros_testX, type="class")
yhat_slgr_vals
as.integer(yhat_slgr_vals)
#type="response" gives the probabilite
yhat_slgr = as.predict(predict(slgr_model, newx = pros_testX, type="class"))
#type="response" gives the probabilite
yhat_slgr = as.numeric(predict(slgr_model, newx = pros_testX, type="class"))
yhat_slgr
yhat_slgr = (yhat_slgr_prob>0.5)
#type="response" gives the probability
yhat_slgr_prob = as.numeric(predict(slgr_model, newx = pros_testX, type="response"))
yhat_slgr = (yhat_slgr_prob>0.5)
yhat_slgr
yhat_slgr = as.numeric((yhat_slgr_prob>0.5))
yhat_slgr
#testing the accuracy of the model
corr_pred_slgr = sum(yhat_slgr == pros_testY)
corr_pred_slgr
corr_pred_slgr/102
#using the same dataset and partitions created for the sparse logistic regression
slg_model_cv = glmnet(pros_trainX, pros_trainY, family="binomial")
#using the same dataset and partitions created for the sparse logistic regression
slg_model = glmnet(pros_trainX, pros_trainY, family="binomial")
summary(slgr_model)
#type="response" gives the probability
yhat_slgr_prob = (predict(slgr_model, newx = pros_testX, type="response")
#type="response" gives the probability
yhat_slgr_prob = (predict(slgr_model, newx = pros_testX, type="response")
#type="response" gives the probability
yhat_slgr_prob = predict(slgr_model, newx = pros_testX, type="response")
yhat_slgr = as.numeric((yhat_slgr_prob>0.5))
yhat_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)
corr_pred_slgr/102
#using the same dataset and partitions created for the sparse logistic regression
slg_model = glmnet(pros_trainX, pros_trainY, family="binomial")
summary(slgr_model)
corr_pred_slgr/nrow(pros_testY)
corr_pred_slgr/
nrow(pros_testY)
corr_pred_slgr/
length(pros_testY)
corr_pred_slgr/
dim(pros_testY)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 70)
train_pros_idx
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
typeof(pros_trainY)
pros_trainY
pros_testY
pros_trainY
corr_pred_slgr/32
corr_pred_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)/32
corr_pred_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)/32
corr_pred_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)
corr_pred_slgr
#type="response" gives the probability
yhat_slgr_prob = predict(slgr_model, newx = pros_testX, type="response")
yhat_slgr = as.numeric((yhat_slgr_prob>0.5))
yhat_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)
corr_pred_slgr
corr_pred_slgr/32
yhat_lgr = as.numeric((yhat_lgr_prob > 0.5))
#predicting values using this model
yhat_lgr_prob = predict(slgr_model, newx = pros_testX, type="response")
yhat_lgr = as.numeric((yhat_lgr_prob > 0.5))
yhat_lgr
#testing the accuracy of this model
corr_pred_lgr = sum(yhat_lgr == pros_testY)
corr_pred_lgr/32
#predicting values using this model
yhat_lgr_prob = predict(slg_model, newx = pros_testX, type="response")
yhat_lgr = as.numeric((yhat_lgr_prob > 0.5))
yhat_lgr
#using the same dataset and partitions created for the sparse logistic regression
slg_model = glmnet(pros_trainX, pros_trainY, family="binomial")
summary(slgr_model)
#predicting values using this model
yhat_lgr_prob = predict(slg_model, newx = pros_testX, type="response")
yhat_lgr = as.numeric((yhat_lgr_prob > 0.5))
yhat_lgr
yhat_lgr_prob
#using the same dataset and partitions created for the sparse logistic regression
slg_model = glm(pros_trainX, pros_trainY, family="binomial")
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 70)
train_pros_idx
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
#using the same dataset and partitions created for the sparse logistic regression
slg_model = glmnet(pros_trainX, pros_trainY, family="binomial")
summary(slg_model)
library("fda.usc")
library("pls")
library("ggplot2")
library(glmnet)
library("plsr")
library("spls")
library("tidyverse")
library("caret")
library(ROCR)
#importing the dataset
concrete_data = read.table("slump_test.data")
#importing the dataset
concrete_data = load("slump_test.data")
#importing the dataset
concrete_data = read_table("slump_test.data",fileEncoding="UTF-16", dec=",")
#importing the dataset
concrete_data = read_table("slump_test.data")
concrete_data
#importing the dataset
concrete_data = read_table("slump_test.data",col_names = "slump_test.names")
concrete_data
#importing the dataset
concrete_data = read.csv("slump_test.csv")
concrete_data
#drop the X column
concrete_data = subset(concrete_data, select = -c("X"))
#drop the X column
concrete_data = subset(concrete_data, select = -c(X))
head(concrete_data)
#importing the dataset
concrete_data = read.csv("slump_test.csv")
concrete_data
#drop the X column
concrete_data = subset(concrete_data, select = -c(X, No))
head(concrete_data)
library("corrplot")
library("moderndive")
library("ggplot2")
library(glmnet)
library("tidyverse")
library("ppcor")
library("corrplot")
#importing the dataset
data = read_xls("cps98.xls")
library("readxl")
#importing the dataset
data = read_xls("cps98.xls")
dim(data)
head(data)
model_1 = lm(formula = ahe ~ age + female + bachelor, data = data)
summary_table_1 = get_regression_table(model_1)
summary_table_1
pred_1 = get_regression_points(model = model_1)
pred_1
model_1 = lm(formula = ahe ~ age + female + bachelor, data = data)
summary_table_1 = get_regression_table(model_1)
summary_table_1
pred_1 = get_regression_points(model = model_1)
pred_1
data["lnahe"] = ln(data["ahe"])
ln(1)
log(exp(1))
log(exp(2))
data["lnahe"] = log(data["ahe"])
head(data)
model_2 = lm(formula = lnahe ~ age + female + bachelor, data = data)
summary_table_2 = get_regression_table(model_2)
summary_table_2
model_3 = lm(formula = lnahe ~ lnage + female + bachelor, data= data)
data["lnage"] = log(data["age"])
model_3 = lm(formula = lnahe ~ lnage + female + bachelor, data= data)
data["lnage"] = log(data["age"])
model_3 = lm(formula = lnahe ~ lnage + female + bachelor, data= data)
summary_table_3 = get_regression_table(model_3)
summary_table_3
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
model_4 = lm(formula = lnahe ~ agexfemale + age + female + bachelor, data=data)
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
model_4 = lm(formula = lnahe ~ agexfem + age + female + bachelor, data=data)
summary_table_4 = get_regression_table(model_4)
summary_table_4
model_4 = lm(formula = lnahe ~ agexfem + age + bachelor, data=data)
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
model_4 = lm(formula = lnahe ~ agexfem + age + bachelor, data=data)
summary_table_4 = get_regression_table(model_4)
summary_table_4
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
model_4 = lm(formula = lnahe ~ agexfem + age + female + bachelor, data=data)
summary_table_4 = get_regression_table(model_4)
summary_table_4
summary(model_4)
data["age^2xfem"] = data["age"]^2*data["female"]
data
data["age2xfem"] = data["age"]^2*data["female"]
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
data["age2xfem"] = data["age"]^2*data["female"]
data
model_4 = lm(formula = lnahe ~ agexfem + age + female + bachelor + age2xfem, data=data)
summary_table_4 = get_regression_table(model_4)
summary(model_4)
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["female"]
data
model_5 = lm(formula = lnahe ~ agexfem + age + female + bachelor + age2xfem + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
summary(model_5)
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["female"]
data
model_5 = lm(formula = lnahe ~  age + female + bachelor + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
summary(model_5)
install.packages("stargazer")
library("stargazer")
model_4c = lm(formula = lnahe ~ agexfem + age + female + age2xfem, data = data)
stargazer(model_4, model_4c, type="text", title = "Regression Table")
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["female"]
data
model_5 = lm(formula = lnahe ~  age + female + bachelor + agexbach + age2xbach, data=data)
model_5c = lm(formula = lnahe ~  age + bachelor + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
summary(model_5)
stargazer(model_5, model_5c, type="text", title = "Regression Table")
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["bachelor"]
data
model_5 = lm(formula = lnahe ~  age + female + bachelor + agexbach + age2xbach, data=data)
model_5c = lm(formula = lnahe ~  age + bachelor + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
summary(model_5)
stargazer(model_5, model_5c, type="text", title = "Regression Table")
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["bachelor"]
data
model_5 = lm(formula = lnahe ~  age + female + bachelor + agexbach + age2xbach, data=data)
model_5c = lm(formula = lnahe ~  age + bachelor + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
library("moderndive")
library("ggplot2")
library(glmnet)
library("tidyverse")
library("ppcor")
library("corrplot")
library("readxl")
library("stargazer")
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexbach"] = data["age"]*data["bachelor"]
data["age2xbach"] = data["age"]^2*data["bachelor"]
data
model_5 = lm(formula = lnahe ~  age + female + bachelor + agexbach + age2xbach, data=data)
model_5c = lm(formula = lnahe ~  age + bachelor + agexbach + age2xbach, data=data)
summary_table_5 = get_regression_table(model_5)
summary(model_5)
stargazer(model_5, model_5c, type="text", title = "Regression Table")
#we can use columns such as agexfemale and age^2xfemale to determine the relation between age and the gender.
data["agexfem"] = data["age"]*data["female"]
data["age2xfem"] = data["age"]^2*data["female"]
data
model_4 = lm(formula = lnahe ~ agexfem + age + female + bachelor + age2xfem, data=data)
model_4c = lm(formula = lnahe ~ agexfem + age + female + age2xfem, data = data)
summary_table_4 = get_regression_table(model_4)
summary(model_4)
stargazer(model_4, model_4c, type="text", title = "Regression Table")
library("spls")
library("plsr")
library("tidyverse")
library("caret")
library("glmnet")
data("prostate")
dim(pros_data)
pros_data = prostate
dim(pros_data)
.libPaths()
prostate.load()
data("prostate")
x_pros = prostate$x
dim(x_pros)
x_pros = prostate$x
y_pros = prostate$y
dim(x_pros)
data("prostate")
x_pros = prostate$x
y_pros = prostate$y
dim(x_pros)
dim(y_pros)
data("prostate")
x_pros = prostate$x
y_pros = prostate$y
dim(x_pros)
length(y_pros)
#splitting into training and testing
prostate = na.omit(prostate)
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 80)
train_pros_idx
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
#splitting into training and testing
prostate = na.omit(prostate)
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 80)
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
#splitting into training and testing
prostate = na.omit(prostate)
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 80)
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
head(pros_trainX)
#splitting into training and testing
prostate = na.omit(prostate)
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 80)
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
install.packages(rpart)
# Q2
```{r, eval=FALSE}
install.packages("rpart")
```
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("fda.usc")
library("fda.usc")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("ipred")
library("spls")
library("plsr")
library("tidyverse")
library("caret")
library("glmnet")
library("rpart")
library("rpart.plot")
library("ipred")
data("prostate")
x_pros = prostate$x
y_pros = prostate$y
dim(x_pros)
length(y_pros)
#splitting into training and testing
set.seed(18)
train_pros_idx = sample(seq_len(nrow(prostate$x)), size = 85)
pros_trainX = prostate$x[train_pros_idx,]
pros_testX = prostate$x[-train_pros_idx,]
pros_trainY = prostate$y[train_pros_idx]
pros_testY = prostate$y[-train_pros_idx]
train_cmb = cbind(pros_trainX, pros_trainY)
train_cmb = as.data.frame(train_cmb)
names(train_cmb)[6034] <- "Y"
train_cmb[1:4,6020:6034]
rpartModel_pros<-rpart(Y~.,data=train_cmb, method = "anova", control = list(cp = 0.001, xval = 10))
#plot(rpartModel_pros)
#text(rpartModel_pros,pretty=TRUE,family="Helvetica")
opar <- par() # to reset later
par(xpd=TRUE)
plot(rpartModel_pros)
text(rpartModel_pros, use.n = T)
par <- opar # restore old setting
print(rpartModel_pros)
pros_testX1 = as.data.frame(pros_testX)
pros_predict<-predict(rpartModel_pros,newdata=pros_testX1)
pros_threshold=.8
is_pros<-(pros_predict>pros_threshold)
actual_pros<-pros_testY
matched<-(is_pros==actual_pros)
length_Y<-length(actual_pros)
misClassif<-1-sum(matched)/length_Y
misClassif
accuracy <- 1-misClassif
accuracy
###Comparing this to other models that we have - sparse logistic regression
#trying out the sparse model (without the lambda)
slg_model_cv = cv.glmnet(pros_trainX, pros_trainY, family="binomial",alpha=1)
#using this cross-validation to find the optimal lambda
optimal_lambda_slgr = slg_model_cv$lambda.min
optimal_lambda_slgr
#using the optimal value of lambda to build the classifier
slgr_model = glmnet(pros_trainX, pros_trainY, alpha = 1, family = "binomial",
lambda = optimal_lambda_slgr)
#type="response" gives the probability
yhat_slgr_prob = predict(slgr_model, newx = pros_testX, type="response")
yhat_slgr = as.numeric((yhat_slgr_prob>0.5))
yhat_slgr
#testing the accuracy of the model by diving the number of correct predictions by number of rows
corr_pred_slgr = sum(yhat_slgr == pros_testY)
accuracy_slgr = corr_pred_slgr/length(pros_testY)
accuracy_slgr
set.seed(123)
#train the bagged model
pros_bag1 = bagging(formula = Y ~ ., data = train_cmb, nbagg = 100, coob = TRUE, control = rpart.control(cp = 0))
set.seed(123)
#train the bagged model
pros_bag1 = bagging(formula = Y ~ ., data = train_cmb, nbagg = 50, coob = TRUE, control = rpart.control(cp = 0))
